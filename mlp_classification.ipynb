{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaVasForce/WebEng19_group7_WeTec/blob/master/mlp_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYc8A1mdRS6X",
        "colab_type": "text"
      },
      "source": [
        "# Train a **Multi-Layer perceptron** with Scikit-learn\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Define the data matrix `x`, and target vector `t`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0yfin0-1pe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = np.array([[-0.09811875,  0.00505513],\n",
        "       [ 0.31775296,  0.09374362],\n",
        "       [-0.60743173, -0.05164308],\n",
        "       [ 0.24169836,  0.30361729],\n",
        "       [ 0.16350231,  0.48120993],\n",
        "       [-0.22262674, -0.12687734],\n",
        "       [-0.13672638,  0.67629236],\n",
        "       [-0.00530367,  0.01760136],\n",
        "       [ 0.91916946,  0.68351717],\n",
        "       [ 1.08691954,  1.07077174],\n",
        "       [ 1.27749763,  0.69683987],\n",
        "       [ 0.94336947,  1.09481271],\n",
        "       [ 0.98010333,  1.03414943],\n",
        "       [ 0.84059644,  1.10322177],\n",
        "       [ 1.14197684,  0.71569053],\n",
        "       [ 1.07461102,  1.41459958],\n",
        "       [ 0.71089468,  0.22767869],\n",
        "       [ 1.03087438, -0.24667113],\n",
        "       [ 1.0620007 , -0.28240629],\n",
        "       [ 1.09046082,  0.13308071],\n",
        "       [ 1.37547994,  0.39220938],\n",
        "       [ 1.36045695, -0.33552651],\n",
        "       [ 0.97213049,  0.21573325],\n",
        "       [ 1.03657274, -0.62158689],\n",
        "       [-0.02634372,  1.20832042],\n",
        "       [ 0.15965234,  1.05627601],\n",
        "       [-0.38782598,  0.89184741],\n",
        "       [-0.51064877,  1.07689404],\n",
        "       [-0.02643556,  1.30400127],\n",
        "       [ 0.06479049,  1.11419167],\n",
        "       [-0.30705681,  1.37559984],\n",
        "       [-0.21194865,  1.01522276]])\n",
        "t=np.array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
        "       -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
        "        1.,  1.,  1.,  1.,  1.,  1.])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfsRkinBT24y",
        "colab_type": "text"
      },
      "source": [
        "**Plot** the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NBsbb4DTbjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a11a11e3-700c-41e9-9884-c6ccabb39c96"
      },
      "source": [
        "plt.plot(x[:16,0],x[:16,1],'ro', label='Class -1')\n",
        "plt.plot(x[16:,0],x[16:,1],'bo', label='Class 1')\n",
        "plt.legend(loc='lower left', frameon=True, facecolor='white', shadow=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6d1b355ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnBjMzKyAE4OZmRVwYjAzswL/Hw8pUVM02hvnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCAcrvkhRrng",
        "colab_type": "text"
      },
      "source": [
        "Define a **MLPCassifier** object with the parameters you want"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjRmeXE_RpfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden = 20  # number of hidden units\n",
        "MAXEPOCHS = 1000 # number of epochs\n",
        "beta = 0.01      # Learning rate\n",
        "activation = 'tanh'  # activation function\n",
        "model = MLPClassifier(hidden_layer_sizes=[num_hidden], activation=activation,\n",
        "                      learning_rate_init=beta, max_iter=MAXEPOCHS, verbose=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9D_LHn4WOMH",
        "colab_type": "text"
      },
      "source": [
        "**Split** the data set randomly into 25% test and 75% training samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C-SMfv14MtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain, xtest, ttrain, ttest = train_test_split(x, t, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DozmR011WgEK",
        "colab_type": "text"
      },
      "source": [
        "**Train** the model on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuOI1j06WceJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5145
        },
        "outputId": "8c56ceb3-0e93-4ba0-b5bd-d7aeb0b70f8b"
      },
      "source": [
        "model.fit(xtrain, ttrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.68260000\n",
            "Iteration 2, loss = 0.67643610\n",
            "Iteration 3, loss = 0.67279460\n",
            "Iteration 4, loss = 0.67046855\n",
            "Iteration 5, loss = 0.66842066\n",
            "Iteration 6, loss = 0.66626063\n",
            "Iteration 7, loss = 0.66405029\n",
            "Iteration 8, loss = 0.66196067\n",
            "Iteration 9, loss = 0.66006527\n",
            "Iteration 10, loss = 0.65829966\n",
            "Iteration 11, loss = 0.65653267\n",
            "Iteration 12, loss = 0.65463711\n",
            "Iteration 13, loss = 0.65253615\n",
            "Iteration 14, loss = 0.65021822\n",
            "Iteration 15, loss = 0.64771875\n",
            "Iteration 16, loss = 0.64508566\n",
            "Iteration 17, loss = 0.64234850\n",
            "Iteration 18, loss = 0.63950485\n",
            "Iteration 19, loss = 0.63652726\n",
            "Iteration 20, loss = 0.63338311\n",
            "Iteration 21, loss = 0.63005319\n",
            "Iteration 22, loss = 0.62653811\n",
            "Iteration 23, loss = 0.62285098\n",
            "Iteration 24, loss = 0.61900327\n",
            "Iteration 25, loss = 0.61499338\n",
            "Iteration 26, loss = 0.61080430\n",
            "Iteration 27, loss = 0.60641018\n",
            "Iteration 28, loss = 0.60178634\n",
            "Iteration 29, loss = 0.59691652\n",
            "Iteration 30, loss = 0.59179393\n",
            "Iteration 31, loss = 0.58641714\n",
            "Iteration 32, loss = 0.58078457\n",
            "Iteration 33, loss = 0.57489120\n",
            "Iteration 34, loss = 0.56872926\n",
            "Iteration 35, loss = 0.56229149\n",
            "Iteration 36, loss = 0.55557453\n",
            "Iteration 37, loss = 0.54857997\n",
            "Iteration 38, loss = 0.54131286\n",
            "Iteration 39, loss = 0.53377892\n",
            "Iteration 40, loss = 0.52598235\n",
            "Iteration 41, loss = 0.51792579\n",
            "Iteration 42, loss = 0.50961223\n",
            "Iteration 43, loss = 0.50104745\n",
            "Iteration 44, loss = 0.49224148\n",
            "Iteration 45, loss = 0.48320833\n",
            "Iteration 46, loss = 0.47396472\n",
            "Iteration 47, loss = 0.46452872\n",
            "Iteration 48, loss = 0.45491934\n",
            "Iteration 49, loss = 0.44515706\n",
            "Iteration 50, loss = 0.43526461\n",
            "Iteration 51, loss = 0.42526706\n",
            "Iteration 52, loss = 0.41519102\n",
            "Iteration 53, loss = 0.40506324\n",
            "Iteration 54, loss = 0.39490969\n",
            "Iteration 55, loss = 0.38475538\n",
            "Iteration 56, loss = 0.37462474\n",
            "Iteration 57, loss = 0.36454194\n",
            "Iteration 58, loss = 0.35453063\n",
            "Iteration 59, loss = 0.34461331\n",
            "Iteration 60, loss = 0.33481090\n",
            "Iteration 61, loss = 0.32514267\n",
            "Iteration 62, loss = 0.31562670\n",
            "Iteration 63, loss = 0.30628004\n",
            "Iteration 64, loss = 0.29711863\n",
            "Iteration 65, loss = 0.28815678\n",
            "Iteration 66, loss = 0.27940688\n",
            "Iteration 67, loss = 0.27087945\n",
            "Iteration 68, loss = 0.26258335\n",
            "Iteration 69, loss = 0.25452591\n",
            "Iteration 70, loss = 0.24671278\n",
            "Iteration 71, loss = 0.23914787\n",
            "Iteration 72, loss = 0.23183332\n",
            "Iteration 73, loss = 0.22476972\n",
            "Iteration 74, loss = 0.21795635\n",
            "Iteration 75, loss = 0.21139131\n",
            "Iteration 76, loss = 0.20507146\n",
            "Iteration 77, loss = 0.19899251\n",
            "Iteration 78, loss = 0.19314914\n",
            "Iteration 79, loss = 0.18753523\n",
            "Iteration 80, loss = 0.18214407\n",
            "Iteration 81, loss = 0.17696848\n",
            "Iteration 82, loss = 0.17200094\n",
            "Iteration 83, loss = 0.16723362\n",
            "Iteration 84, loss = 0.16265860\n",
            "Iteration 85, loss = 0.15826793\n",
            "Iteration 86, loss = 0.15405371\n",
            "Iteration 87, loss = 0.15000808\n",
            "Iteration 88, loss = 0.14612329\n",
            "Iteration 89, loss = 0.14239172\n",
            "Iteration 90, loss = 0.13880598\n",
            "Iteration 91, loss = 0.13535894\n",
            "Iteration 92, loss = 0.13204371\n",
            "Iteration 93, loss = 0.12885369\n",
            "Iteration 94, loss = 0.12578261\n",
            "Iteration 95, loss = 0.12282448\n",
            "Iteration 96, loss = 0.11997367\n",
            "Iteration 97, loss = 0.11722482\n",
            "Iteration 98, loss = 0.11457289\n",
            "Iteration 99, loss = 0.11201310\n",
            "Iteration 100, loss = 0.10954091\n",
            "Iteration 101, loss = 0.10715209\n",
            "Iteration 102, loss = 0.10484261\n",
            "Iteration 103, loss = 0.10260868\n",
            "Iteration 104, loss = 0.10044674\n",
            "Iteration 105, loss = 0.09835341\n",
            "Iteration 106, loss = 0.09632552\n",
            "Iteration 107, loss = 0.09436007\n",
            "Iteration 108, loss = 0.09245424\n",
            "Iteration 109, loss = 0.09060538\n",
            "Iteration 110, loss = 0.08881098\n",
            "Iteration 111, loss = 0.08706870\n",
            "Iteration 112, loss = 0.08537634\n",
            "Iteration 113, loss = 0.08373181\n",
            "Iteration 114, loss = 0.08213318\n",
            "Iteration 115, loss = 0.08057862\n",
            "Iteration 116, loss = 0.07906639\n",
            "Iteration 117, loss = 0.07759488\n",
            "Iteration 118, loss = 0.07616255\n",
            "Iteration 119, loss = 0.07476796\n",
            "Iteration 120, loss = 0.07340975\n",
            "Iteration 121, loss = 0.07208664\n",
            "Iteration 122, loss = 0.07079741\n",
            "Iteration 123, loss = 0.06954091\n",
            "Iteration 124, loss = 0.06831605\n",
            "Iteration 125, loss = 0.06712179\n",
            "Iteration 126, loss = 0.06595714\n",
            "Iteration 127, loss = 0.06482119\n",
            "Iteration 128, loss = 0.06371302\n",
            "Iteration 129, loss = 0.06263179\n",
            "Iteration 130, loss = 0.06157669\n",
            "Iteration 131, loss = 0.06054695\n",
            "Iteration 132, loss = 0.05954181\n",
            "Iteration 133, loss = 0.05856056\n",
            "Iteration 134, loss = 0.05760251\n",
            "Iteration 135, loss = 0.05666700\n",
            "Iteration 136, loss = 0.05575339\n",
            "Iteration 137, loss = 0.05486106\n",
            "Iteration 138, loss = 0.05398940\n",
            "Iteration 139, loss = 0.05313785\n",
            "Iteration 140, loss = 0.05230584\n",
            "Iteration 141, loss = 0.05149282\n",
            "Iteration 142, loss = 0.05069827\n",
            "Iteration 143, loss = 0.04992167\n",
            "Iteration 144, loss = 0.04916252\n",
            "Iteration 145, loss = 0.04842035\n",
            "Iteration 146, loss = 0.04769468\n",
            "Iteration 147, loss = 0.04698505\n",
            "Iteration 148, loss = 0.04629102\n",
            "Iteration 149, loss = 0.04561215\n",
            "Iteration 150, loss = 0.04494803\n",
            "Iteration 151, loss = 0.04429825\n",
            "Iteration 152, loss = 0.04366241\n",
            "Iteration 153, loss = 0.04304013\n",
            "Iteration 154, loss = 0.04243103\n",
            "Iteration 155, loss = 0.04183476\n",
            "Iteration 156, loss = 0.04125097\n",
            "Iteration 157, loss = 0.04067931\n",
            "Iteration 158, loss = 0.04011945\n",
            "Iteration 159, loss = 0.03957108\n",
            "Iteration 160, loss = 0.03903389\n",
            "Iteration 161, loss = 0.03850757\n",
            "Iteration 162, loss = 0.03799185\n",
            "Iteration 163, loss = 0.03748644\n",
            "Iteration 164, loss = 0.03699106\n",
            "Iteration 165, loss = 0.03650546\n",
            "Iteration 166, loss = 0.03602938\n",
            "Iteration 167, loss = 0.03556257\n",
            "Iteration 168, loss = 0.03510480\n",
            "Iteration 169, loss = 0.03465583\n",
            "Iteration 170, loss = 0.03421544\n",
            "Iteration 171, loss = 0.03378341\n",
            "Iteration 172, loss = 0.03335954\n",
            "Iteration 173, loss = 0.03294361\n",
            "Iteration 174, loss = 0.03253543\n",
            "Iteration 175, loss = 0.03213482\n",
            "Iteration 176, loss = 0.03174158\n",
            "Iteration 177, loss = 0.03135553\n",
            "Iteration 178, loss = 0.03097650\n",
            "Iteration 179, loss = 0.03060433\n",
            "Iteration 180, loss = 0.03023884\n",
            "Iteration 181, loss = 0.02987989\n",
            "Iteration 182, loss = 0.02952731\n",
            "Iteration 183, loss = 0.02918095\n",
            "Iteration 184, loss = 0.02884068\n",
            "Iteration 185, loss = 0.02850635\n",
            "Iteration 186, loss = 0.02817782\n",
            "Iteration 187, loss = 0.02785497\n",
            "Iteration 188, loss = 0.02753765\n",
            "Iteration 189, loss = 0.02722576\n",
            "Iteration 190, loss = 0.02691916\n",
            "Iteration 191, loss = 0.02661774\n",
            "Iteration 192, loss = 0.02632138\n",
            "Iteration 193, loss = 0.02602997\n",
            "Iteration 194, loss = 0.02574341\n",
            "Iteration 195, loss = 0.02546158\n",
            "Iteration 196, loss = 0.02518439\n",
            "Iteration 197, loss = 0.02491173\n",
            "Iteration 198, loss = 0.02464352\n",
            "Iteration 199, loss = 0.02437964\n",
            "Iteration 200, loss = 0.02412001\n",
            "Iteration 201, loss = 0.02386455\n",
            "Iteration 202, loss = 0.02361316\n",
            "Iteration 203, loss = 0.02336576\n",
            "Iteration 204, loss = 0.02312226\n",
            "Iteration 205, loss = 0.02288259\n",
            "Iteration 206, loss = 0.02264667\n",
            "Iteration 207, loss = 0.02241441\n",
            "Iteration 208, loss = 0.02218575\n",
            "Iteration 209, loss = 0.02196061\n",
            "Iteration 210, loss = 0.02173893\n",
            "Iteration 211, loss = 0.02152062\n",
            "Iteration 212, loss = 0.02130564\n",
            "Iteration 213, loss = 0.02109390\n",
            "Iteration 214, loss = 0.02088534\n",
            "Iteration 215, loss = 0.02067991\n",
            "Iteration 216, loss = 0.02047753\n",
            "Iteration 217, loss = 0.02027816\n",
            "Iteration 218, loss = 0.02008173\n",
            "Iteration 219, loss = 0.01988819\n",
            "Iteration 220, loss = 0.01969748\n",
            "Iteration 221, loss = 0.01950954\n",
            "Iteration 222, loss = 0.01932433\n",
            "Iteration 223, loss = 0.01914179\n",
            "Iteration 224, loss = 0.01896187\n",
            "Iteration 225, loss = 0.01878453\n",
            "Iteration 226, loss = 0.01860971\n",
            "Iteration 227, loss = 0.01843736\n",
            "Iteration 228, loss = 0.01826745\n",
            "Iteration 229, loss = 0.01809992\n",
            "Iteration 230, loss = 0.01793474\n",
            "Iteration 231, loss = 0.01777186\n",
            "Iteration 232, loss = 0.01761123\n",
            "Iteration 233, loss = 0.01745283\n",
            "Iteration 234, loss = 0.01729660\n",
            "Iteration 235, loss = 0.01714251\n",
            "Iteration 236, loss = 0.01699051\n",
            "Iteration 237, loss = 0.01684059\n",
            "Iteration 238, loss = 0.01669268\n",
            "Iteration 239, loss = 0.01654677\n",
            "Iteration 240, loss = 0.01640281\n",
            "Iteration 241, loss = 0.01626077\n",
            "Iteration 242, loss = 0.01612062\n",
            "Iteration 243, loss = 0.01598233\n",
            "Iteration 244, loss = 0.01584586\n",
            "Iteration 245, loss = 0.01571118\n",
            "Iteration 246, loss = 0.01557826\n",
            "Iteration 247, loss = 0.01544706\n",
            "Iteration 248, loss = 0.01531758\n",
            "Iteration 249, loss = 0.01518976\n",
            "Iteration 250, loss = 0.01506359\n",
            "Iteration 251, loss = 0.01493903\n",
            "Iteration 252, loss = 0.01481606\n",
            "Iteration 253, loss = 0.01469466\n",
            "Iteration 254, loss = 0.01457479\n",
            "Iteration 255, loss = 0.01445643\n",
            "Iteration 256, loss = 0.01433956\n",
            "Iteration 257, loss = 0.01422415\n",
            "Iteration 258, loss = 0.01411018\n",
            "Iteration 259, loss = 0.01399763\n",
            "Iteration 260, loss = 0.01388646\n",
            "Iteration 261, loss = 0.01377667\n",
            "Iteration 262, loss = 0.01366822\n",
            "Iteration 263, loss = 0.01356110\n",
            "Iteration 264, loss = 0.01345528\n",
            "Iteration 265, loss = 0.01335075\n",
            "Iteration 266, loss = 0.01324748\n",
            "Iteration 267, loss = 0.01314545\n",
            "Iteration 268, loss = 0.01304465\n",
            "Iteration 269, loss = 0.01294505\n",
            "Iteration 270, loss = 0.01284664\n",
            "Iteration 271, loss = 0.01274939\n",
            "Iteration 272, loss = 0.01265330\n",
            "Iteration 273, loss = 0.01255833\n",
            "Iteration 274, loss = 0.01246448\n",
            "Iteration 275, loss = 0.01237173\n",
            "Iteration 276, loss = 0.01228005\n",
            "Iteration 277, loss = 0.01218944\n",
            "Iteration 278, loss = 0.01209988\n",
            "Iteration 279, loss = 0.01201134\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "       hidden_layer_sizes=[20], learning_rate='constant',\n",
              "       learning_rate_init=0.01, max_iter=1000, momentum=0.9,\n",
              "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "       validation_fraction=0.1, verbose=5, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n-2SeEeRdrR",
        "colab_type": "text"
      },
      "source": [
        "Model **prediction** on the *test data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-OnbtynWV9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "061478c4-8f12-4a96-8559-cc1aee216153"
      },
      "source": [
        "ytest = model.predict(xtest)\n",
        "test_accuracy = float(np.sum(ytest==ttest))/len(ttest)\n",
        "print('Test Accuracy = {}%'.format(test_accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy = 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYH3UHsSIH4",
        "colab_type": "text"
      },
      "source": [
        "Model **prediction** on *training data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kjyrb77Rr0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f15af166-e2e3-49a1-dfb4-ed9b2526e407"
      },
      "source": [
        "ytrain = model.predict(xtrain)\n",
        "train_accuracy = float(np.sum(ytrain==ttrain))/len(ttrain)\n",
        "print('Train Accuracy = {}%'.format(train_accuracy*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy = 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXXN9uSyRlv5",
        "colab_type": "text"
      },
      "source": [
        "Model **coefficients**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCEeQtEX3IQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "0cb5a60d-561c-41ff-af2a-3c0d68d68481"
      },
      "source": [
        "model.coefs_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.15963391,  1.95935391,  1.52753574,  1.1686458 ,  1.4611802 ,\n",
              "         -1.50765806, -1.452607  ,  1.543189  , -1.92759341,  1.61925896,\n",
              "          1.71595811,  1.17055569,  1.6214378 ,  1.10381749, -1.28866733,\n",
              "         -1.67758951, -1.41719575, -1.3184673 , -1.43430151,  0.21357917],\n",
              "        [-1.38504749, -1.97952633,  1.90759879,  1.22509293, -2.22408218,\n",
              "          2.04515171, -1.56448233, -1.94575929, -1.58327913,  1.75454492,\n",
              "          1.79289567,  1.09042236, -1.6354051 , -1.98695262,  1.86685165,\n",
              "          1.82101177,  1.89706113,  1.45475197, -1.33948588, -0.11300917]]),\n",
              " array([[-2.12638524],\n",
              "        [-1.55516509],\n",
              "        [ 2.00077348],\n",
              "        [-2.25296003],\n",
              "        [ 2.13202046],\n",
              "        [-1.82078129],\n",
              "        [ 1.91947446],\n",
              "        [ 1.51347196],\n",
              "        [-1.46672523],\n",
              "        [-2.23751901],\n",
              "        [ 1.70607193],\n",
              "        [-1.5912718 ],\n",
              "        [-1.96116295],\n",
              "        [ 1.85276205],\n",
              "        [-1.86054293],\n",
              "        [ 1.55136796],\n",
              "        [ 2.22906489],\n",
              "        [ 1.56515207],\n",
              "        [-1.73335697],\n",
              "        [-0.31045282]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}